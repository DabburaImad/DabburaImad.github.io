{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a8df91-0b76-4abe-a256-ca7df2fd17dd",
   "metadata": {},
   "source": [
    "---\n",
    "title: Airflow\n",
    "date: \"2022-09-26\"\n",
    "image: feature.png\n",
    "categories: [\"Data Engineering\", \"MLOps\"]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48243cb5-f66b-4d0c-9dc3-fb567d6cc643",
   "metadata": {},
   "source": [
    "![](feature.png){fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f973c3-9876-4255-aca1-f14b351accdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c527d5-47f6-4274-b11b-41983dca4846",
   "metadata": {},
   "source": [
    "**Airflow** is workflow orchestration tool that is written in Python at Airbnb. The workflow is also written in Python. It defines the workflow as a **DAG** so it is easy to determine the dependencies between tasks. If any task failed, we don't need to rerun the workflow again, we can just run the failed task and all the tasks that depend on it. We can also do *backfilling* by running the pipeline/tasks for time intervals in the past.\n",
    "\n",
    "Airflow consists of mainly three components:\n",
    "- **The Airflow scheduler**: Parses DAGs, checks their schedule interval, and (if the DAGs’ schedule has passed) starts scheduling the DAGs’ tasks for execution by passing them to the Airflow workers.\n",
    "- **The Airflow workers**: Pick up tasks that are scheduled for execution and execute them. As such, the workers are responsible for actually “doing the work.”\n",
    "- **The Airflow webserver**: Visualizes the DAGs parsed by the scheduler and provides the main interface for users to monitor DAG runs and their results. It uses the metadata database which has all the logs and other metadata about tasks and workflows.\n",
    "\n",
    "Conceptually, the scheduling algorithm follows the following steps:\n",
    "- For each open (= uncompleted) task in the graph, do the following:\n",
    "    – For each edge pointing toward the task, check if the “upstream” task on the other end of the edge has been completed.\n",
    "    – If all upstream tasks have been completed, add the task under consideration to a queue of tasks to be executed.\n",
    "2. Execute the tasks in the execution queue, marking them completed once they finish performing their work.\n",
    "3. Jump back to step 1 and repeat until all tasks in the graph have been completed.\n",
    "\n",
    "The scheduler in Airflow runs roughly through the following steps:\n",
    "1. Once users have written their workflows as DAGs, the files containing these DAGs are read by the scheduler to extract the corresponding tasks, dependen- cies, and schedule interval of each DAG.\n",
    "2. For each DAG, the scheduler then checks whether the schedule interval for the DAG has passed since the last time it was read. If so, the tasks in the DAG are scheduled for execution.\n",
    "3. For each scheduled task, the scheduler then checks whether the dependencies (= upstream tasks) of the task have been completed. If so, the task is added to the execution queue.\n",
    "4. The scheduler waits for several moments before starting a new loop by jumping back to step 1.\n",
    "\n",
    "<img src=\"images/scheduler-overview.png\">\n",
    "\n",
    "Airflow can be run:\n",
    "1. In python virtual environment\n",
    "2. Inside Docker containers. In this case, Airflow scheduler, webserver, and metastore would run each in separate containers\n",
    "\n",
    "The main disadvantages of Airflow are:\n",
    "1. It can get very messy and hard to understand for complex workflows\n",
    "2. It is best used for batch/recurring jobs NOT streaming jobs\n",
    "3. Mainly support static DAGs and hard to implement dynamic DAGs. Imagine you’re reading from a database and you want to create a step to process each record in the database (e.g. to make a prediction), but you don’t know in advance how many records there are in the database, Airflow won’t be able to handle that.\n",
    "4. It is monolithic, which means it packages the entire workflow into one container. If two different steps in your workflow have different requirements, you can, in theory, create different containers for them using Airflow’s DockerOperator, but it’s not that easy to do so.\n",
    "5. Airflow’s DAGs are not parameterized, which means you can’t pass parameters into your workflows. So if you want to run the same model with different learning rates, you’ll have to create different workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61390489-a395-46bd-b3cc-96e3beb4365e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Airflow DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a74902c-0362-42da-9c40-930c74eb6d99",
   "metadata": {},
   "source": [
    "- `DAG()` class is needed to instantiate a **DAG** which will be the starting point of any workflow.\n",
    "    - The required arguments are: `dag_id` which is the name Airflow web UI uses to display workflow. `start_date` which is when to start running the workflow, it can be in the past\n",
    "    - There are other arguments such as `schedule_interval` which determines the schedule to rerun the DAG\n",
    "- `Operator` is responsible for a piece of work and almost represents a task.\n",
    "    - It has `task_id` which is the name web UI uses to display the task\n",
    "    - There are many operators such as `BashOperator`, `PythonOperator` ... All of them inherits from `BaseOperator`\n",
    "    - Some operators are generic such as `BashOperator` and some are specific such as `EmailOperator`\n",
    "- `Task` is a wrapper/manager over operator that makes sure the operator gets executed\n",
    "- `>>` represents the dependencies between tasks\n",
    "    - `a` >> `b` means `a` should run before `b`\n",
    "- Airflow UI offers two views:\n",
    "    - **tree view** that shows the DAG runs over time. Each column is one run. Each row is a task. So we can inspect status of tasks over time\n",
    "    - **graph view** that shows the DAG as a graph which helps showing the dependencies of tasks in the workflow\n",
    "- If any task failed, all successive tasks that depend on it don't run\n",
    "    - We can rerun the failed tasks (which also would cause successive tasks to rerun) w/o having to rerun the workflow from scratch\n",
    "    - We can inspect the logs to see what was the reason for the errors\n",
    "- Tasks can run in parallel depending on their dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae909111-6107-43af-b84a-c68eb1e2deff",
   "metadata": {},
   "source": [
    "- To setup Airflow locally inside Python virtual env:\n",
    "    - pip install apache-airflow\n",
    "    - airflow init db # Initialize metastore locally using SQLite; not recommended for production\n",
    "    - airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.org # Create user\n",
    "    - airflow webserver # Start web server to use web UI\n",
    "    - airflow scheduler # Start scheduler, don't use sequential in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "135db21e-c3ef-4364-9499-af045aa09635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Task(PythonOperator): python>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load dags/simple-dag.py\n",
    "import airflow\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "f = lambda: print(1)\n",
    "dag = DAG(dag_id=\"simple-workflow\", start_date=airflow.utils.dates.days_ago(10))\n",
    "a = BashOperator(task_id=\"bash\", bash_command=\"echo 'a'\", dag=dag)\n",
    "b = PythonOperator(task_id=\"python\", python_callable=f, dag=dag)\n",
    "a >> b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e36ac4a-0930-4ca2-9a52-ece6173b5d50",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f754eb09-8fdd-422c-898e-6c668b1df904",
   "metadata": {},
   "source": [
    "- Airflow will schedule the first execution of DAG at the end of the interval; which means after the last time point in the interval has passed. For example, if we schedule it to run `@daily`, it will run t midnight of each day starting from the `start_date` until (optionally) `end_date`. In other words, as soon as `23:59:59` has passed which means any time after `00:00:00`.\n",
    "    - Example: if start_date=\"2022-01-01\" and schedule_interval=\"@daily\" -> The first time it runs is any time soon after \"2022-01-02 00:00\" which is midnight of January second.\n",
    "- We can use convenience string (such as `@daily`), timedetla objects (such as timedelta(days=3), or cron expressions (such as `0 0 * * 0` which means weekly on Sunday 00:00)\n",
    "- Frequency scheduling intervals (shorthands):\n",
    "    - `@once`: Schedule once and only once.\n",
    "    - `@hourly`: Run once an hour at the beginning of the hour.\n",
    "    - `@daily`: Run once a day at midnight.\n",
    "    - `@weekly`: Run once a week at midnight on Sunday morning.\n",
    "    - `@monthly`: Run once a month at midnight on the first day of the month. Run once a year at midnight on January 1.\n",
    "\n",
    "- Cron-based intervals:\n",
    "```\n",
    "# ┌─────── minute (0 - 59)\n",
    "# │ ┌────── hour (0 - 23)\n",
    "# │ │ ┌───── dayofthemonth(1-31)\n",
    "# │ │ │ ┌───── month(1-12)\n",
    "# │ │ │ │ ┌──── dayoftheweek(0-6)(SundaytoSaturday; \n",
    "# │ │ │ │ │ 7 is also Sunday on some systems) \n",
    "# * * * * *\n",
    "```\n",
    "    - \"*\" means don't care values.\n",
    "    - Examples:\n",
    "        1. 0**** means hourly\n",
    "        2. 00*** means daily at midnight\n",
    "        3. 00**0 means weekly at midnight on Sunday\n",
    "    - Useful link to check meaning of cron-based intervals: https://crontab.guru/\n",
    "- Cron expressions have limitations when trying to specify frequency-based intervals such as every three days. The reason for this behavior is that cron expressions are stateless and don't look at previous runs to determine next run, they only look at the current time to see if it matches the expression.\n",
    "- Airflow allows us to use frequency-based intervals using `timedelta` from datetime library. This way we can use previous run to determine the next run.\n",
    "    - Example: schedule_interval=\"timedelta(days=3)\" means to run every 3 days after start_date.\n",
    "- We can use dynamic time reference that uses execution dates which allows us to do the work incrementally. Airflow will pass those dates to the tasks to determine which schedule interval is being executed.\n",
    "    - `execution_date` is a timestamp of the start time of the schedule interval\n",
    "    - `next_execution_date` is a timestamp of the end time of the schedule interval\n",
    "    - `previous_execution_date` is a timestamp of the start time of the previous schedule interval\n",
    "    - Airflow uses Jinja-based templating such as {{variable_name}}:\n",
    "        ```bash\n",
    "        fetch_events = BashOperator(\n",
    "            task_id=\"fetch_events\",\n",
    "            bash_command=(\n",
    "                \"mkdir -p /data && \"\n",
    "                \"curl -o /data/events.json \" \"http://localhost:5000/events?\" \n",
    "                \"start_date={{execution_date.strftime('%Y-%m-%d')}}\" \n",
    "                \"&end_date={{next_execution_date.strftime('%Y-%m-%d')}}\"\n",
    "            ),\n",
    "        dag=dag,\n",
    "      )\n",
    "      ```\n",
    "    - Or we can use shorthands:\n",
    "        ```bash\n",
    "        fetch_events = BashOperator(\n",
    "            task_id=\"fetch_events\",\n",
    "            bash_command=(\n",
    "                \"mkdir -p /data && \"\n",
    "                \"curl -o /data/events.json \" \"http://localhost:5000/events?\" \n",
    "                \"start_date={{ds}}\" \n",
    "                \"&end_date={{next_ds}}\"\n",
    "            ),\n",
    "        dag=dag,\n",
    "      )\n",
    "      ```\n",
    "    - `ds` has `YYYY-MM-DD` format while `ds_nodash` has `YYYYMMDD` format\n",
    "    - Shorthands: ds, ds_nodash, next_ds, next_ds_nodash, ps, ps_nodash \n",
    "execution date of the next interval.\n",
    "- We can also use dates or any dynamic parameters to Python function using `templates_dict` argument and the python callable will be passed the context that has the `templates_dict` For example:\n",
    "    ```bash\n",
    "        calculate_stats = PythonOperator(\n",
    "           task_id=\"calculate_stats\",\n",
    "           python_callable=_calculate_stats,\n",
    "           templates_dict={\n",
    "                \"input_path\": \"/data/events/{{ds}}.json\",\n",
    "               \"output_path\": \"/data/stats/{{ds}}.csv\",\n",
    "        },\n",
    "        dag=dag\n",
    "        )\n",
    "    ```\n",
    "    ```python\n",
    "        def _calculate_stats(**context):\n",
    "            \"\"\"Calculates event statistics.\"\"\"\n",
    "                input_path = context[\"templates_dict\"][\"input_path\"] \n",
    "                output_path = context[\"templates_dict\"][\"output_path\"]\n",
    "    ```\n",
    "- Because Airlfow follows **Interval-Based Scheduling**, that means DAGs run only after the last time point of schedule interval passed. If we run the DAG daily starting from `2022-01-01`, the first time it runs is soon after `2022-01-02 00:00:00` has passed and the `execution_date` would be `2022-01-01` even though it is running in `2022-01-02`. This is because it is running for the corresponding interval.\n",
    "    - The end of the previous interval is `execution_date`\n",
    "    - **One caveat for this is that `previous_execution_date` and `next_execution_date` are only defined for DAGs that run on schedule interval. This means that those values are undefined when the DAGs are run from the UI or CLI**\n",
    "- Airflow allows us to have `start_date` in the past. This will help us in backfilling. By default, Airflow will run all the schedule intervals from the past until current time once the DAG is activated. We can control this behavior using `catchup` parameter to the `DAG()` class. If we set it to `False`, it won't run previous schedule intervals.\n",
    "    - Backfilling is also helpful if we change the code for the DAG. It would run all previous schedules after we clear them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c91ef2b-c5c0-422c-bbe4-6b521f22428b",
   "metadata": {},
   "source": [
    "- **Best Practices**:\n",
    "    - Task needs to be **atomic** which means a single coherent unit of work. This allows us to split work into smaller units where if one fails we know exactly what is it and recover easily.\n",
    "    - Task needs to be **idempotent** which means it has no side effects on the system when it reruns. If the task is given the same input, it should produce the same output.\n",
    "        - In database systems, we can use upsert, which allows us to overwrite existing row.\n",
    "        - When writing to files, make sure that rerunning the same task for the same interval don't write data again. *Append* doesn't let us make the task idempotent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44522eb5-488a-40f7-9818-ac024fae4311",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Task Context & Jinja Templating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c0c779-09bb-4463-9887-de030b466065",
   "metadata": {},
   "source": [
    "- Airflow uses `Pendulum` library for datetimes. It is a drop-in replacement to the Python standard library datetime but with much nicer API and more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c506795-4bd5-486d-a657-4b9a8c784db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Date(2022, 2, 11), 9, 5, Date(2022, 2, 21))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pendulum\n",
    "now = pendulum.now()\n",
    "now.date(), now.hour, now.day_of_week, now.add(days=10).date()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1782d844-da6c-454b-bef1-39b191fe71eb",
   "metadata": {},
   "source": [
    "- Not all arguments can be templates. By default, all arguments are not made into templates and `{{name}}` will be read as a literal string `name` unless it is included in `template_fields` in the list of attributes that can be templated in the Operator.\n",
    "    - Elements in the `template_fields` are names for class attributes. The arguments passed to the `__init__` match the class attributes.\n",
    "- All operators; such as BashOperator, take their argument as string except PythonOperator. It takes its argument as `python_callable`, which is any callable object in Python. The context and parameters will be available to this callable.\n",
    "    - The context variable is a dictionary that has all the instance variables for this task.\n",
    "        - we can use default `**kwargs` or make it easier to read using `**context`\n",
    "    - If we specify argument name in the python_callable, then Airflow will call the python_callable with all the variables in the context.\n",
    "        - If a variable is specified as argument by the callable, then it is passed to the callabe\n",
    "        - Otherwise, it is added to the context dictionary. If we don't have context dictionary as an argument for the callable, then all other variables in the context that are not specified as arguments will be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ede4686f-e66c-431d-9155-6e89a7031544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load dags/python-context-dag.py\n",
    "import airflow\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "dag = DAG(dag_id=\"python-operator-context\", start_date=airflow.utils.dates.days_ago(1))\n",
    "\n",
    "\n",
    "def _print_context(**kwargs):\n",
    "    print(kwargs)\n",
    "print_context = PythonOperator(task_id=\"print-context\", python_callable=_print_context, dag=dag)\n",
    "print_context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cafd9d-6ac5-47bd-82ca-ae370b90d444",
   "metadata": {},
   "source": [
    "- Some arguments of operators can be templated\n",
    "- Templating happens at run time\n",
    "- We can provide arguments to PythonOperator using:\n",
    "    - `op_args`: list of positional arguments that are passed to the callable\n",
    "    - `op_kwargs`: dictionary of keyword arguments\n",
    "- We can inspect the templated arguments either on the UI or using the CLI:\n",
    "    - CLI: `airflow tasks render [dag id] [task id] [desired execution date]`\n",
    "- There are two ways to pass data between tasks:\n",
    "    1. read/write to the metastore. It is called `XCom`\n",
    "        - This is done by pickling the objects we want to share and write it to metastore. After that, tasks can read the pickled objects (and unpickle them)\n",
    "        - This is only recommended for small objects because the object are stored as blobs in the metastore. Don't use it for large objects\n",
    "    2. read/write to persistent storage such as disk or database\n",
    "- Tasks are independent and may run on completely different machines -> Can't share memory -> Sharing has to be through persistent storage.\n",
    "- Most operators are installed via separate pip install. For example, PostgresOperator allows us to work with PostgreSQL database.\n",
    "    - We can install operators like `pip install apache-airflow-providers-*`\n",
    "    - We can import the operator as `from airflow.providers.pogstres.operators.postgres import PostgresOperator`\n",
    "    - We can add connections using UI or CLI, which Airflow store them encrypted in metastore, such as:\n",
    "\n",
    "```bash\n",
    "airflow connections add \\\n",
    "--conn-type postgres \\\n",
    "--conn-host localhost \\\n",
    "--conn-login postgres \\\n",
    "--conn-password mysecretpassword \\\n",
    "my_postgres\n",
    "```\n",
    "        - We can later refer to those credentions by name when connecting to any database\n",
    "    - Airflow takes care of setting up the connection and close it once done\n",
    "- Postgres is an external system and Airflow supports connecting to a wide range of external systems with the help of many operators in its ecosystem. This does have an implication: connecting to an external system often requires specific dependencies to be installed, which allow connecting and communicating with the external system. This also holds for Postgres; we must install the package apache-airflow-providers- postgres to install additional Postgres dependencies in our Airflow installation.\n",
    "    - Upon execution of the PostgresOperator, a number of things happen. The PostgresOperator will instantiate a so-called hook to communicate with Postgres. The hook deals with creating a connection, sending queries to Postgres and closing the connection afterward. The operator is merely passing through the request from the user to the hook in this situation.\n",
    "    - An operator determines what has to be done; a hook determines how to do something. When building pipelines like these, you will only deal with operators and have no notion of any hooks, because hooks are used internally in operators.\n",
    "- There’s a number of things to point out in this last step. The DAG has an additional argument: template_searchpath. Besides a string INSERT INTO ..., the content of files can also be templated. Each operator can read and template files with specific extensions by providing the file path to the operator. In the case of the Postgres- Operator, the argument SQL can be templated and thus a path to a file holding a SQL query can also be provided. Any filepath ending in .sql will be read, templates in the file will be rendered, and the queries in the file will be executed by the PostgresOperator. Again, refer to the documentation of the operators and check the field template_ext, which holds the file extensions that can be templated by the operator.\n",
    "    - Jinja requires you to provide the path to search for files that can be templated. By default, only the path of the DAG file is searched for, but since we’ve stored it in /tmp, Jinja won’t find it. To add paths for Jinja to search, set the argument template_searchpath on the DAG and Jinja will traverse the default path plus additional provided paths to search for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e883fcb-84af-49a5-8106-2f0e218ce11c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Dependencies Between Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0d2fc7-98b3-4b1d-a734-9a538e8615f2",
   "metadata": {},
   "source": [
    "- Dependencies in airflow is specified using the right shift symbol `>>`. It tells Airflow which tasks should be run first before running other tasks.\n",
    "- Basic Dependenices:\n",
    "    1. **Linear dependenies**: `a >> b >> c`. This means that `a` has to run before `b` which should run before `c`. If any task fails, the downstream task won't run and the errors are propagated to them from preceding tasks. They can only run after the errors are fixed for that interval.\n",
    "    2. **Fan-in/Fan-out dependencies** <img src=\"images/fan-in-out.png\">:\n",
    "        - Fan-in: When 1 task is dependent on >= 2 tasks to run. *join_datasets* is fan-in task. Fan-in tasks can be specified as: `[clean_sales, clean_weather] >> join_datasets`\n",
    "        - Fan-out: When >= 2 tasks are dependent on 1 task to run. *start* is a fan-out task. Fan-out tasks can be specified as: `start >> [fetch_sales, fetch_weather]`\n",
    "        - This is how we can specify dependencies for the DAG in the above picture:\n",
    "        ```python\n",
    "            start >> [fetch_sales, fetch_weather]\n",
    "            fetch_sales >> clean_sales\n",
    "            fetch_weather >> clean_weather\n",
    "            [clean_sales, clean_weather] >> join_datasets\n",
    "            join_datasets >> train_model\n",
    "            train_model >> deploy_model\n",
    "        ```\n",
    "- Branching: \n",
    "    1. We can take care of conditional execution of code paths inside the task, i.e. inside Python script in the case of PythonOperator. Depending on some condition during execution, different code paths and logic will be followed. The main disadvantages of this approach is that 1) it is hard to figure out with code path is being executed on each run from tree/graph view unless we have logging enabled, 2) Adds more complexity to the code structure, 3) May not let us use specialized operators that abstract aways a lot of the boilerplate code such as PostgresOperator. For example, if we have fetch data from either CSV or SQL database depending on condition at execution.\n",
    "    2. We can add `BrachPythonOperator`task that takes a Python callable to determine which tasks to execute next. The Python callable has to return the task_id of the task (or list of task_id) that Airflow should execute next. Example: <img src=\"images/branching-v1.png\">\n",
    "\n",
    "    ```python\n",
    "        def _pick_erp_system(**context):\n",
    "            if context[\"execution_date\"] < ERP_SWITCH_DATE:\n",
    "               return \"fetch_sales_old\"\n",
    "            else:\n",
    "               return \"fetch_sales_new\"\n",
    "\n",
    "        pick_erp_system = BranchPythonOperator(\n",
    "            task_id=\"pick_erp_system\",\n",
    "            python_callable=_pick_erp_system,\n",
    "            )\n",
    "\n",
    "        start >> [pick_erp_system, fetch_weather]\n",
    "        pick_erp_system >> [fetch_sales_old, fetch_sales_new]\n",
    "        fetch_sales_old >> clean_sales_old\n",
    "        fetch_sales_new >> clean_sales_new\n",
    "        fetch_weather >> clean_weather\n",
    "        [clean_sales_old, clean_sales_new, clean_weather] >> join_datasets\n",
    "        join_datasets >> train_model\n",
    "        train_model >> deploy_model\n",
    "\n",
    "    ```\n",
    "    - Since downstream tasks only get scheduled & executed if all thier downstream tasks finished successfully, `jon_datasets` task will never success because with the above dependency either `clean_sales_old` or `clean_sales_new` would execute BUT NOT BOTH. We can adjust this using `trigger_rule` argument (default is `\"all_success\"` in the operatror by specifying `\"non_failed\"`. This will run downstream task if all downstream tasks haven't failed even if they never executed. Therefore, we can change trigger_rule for `join_datasest` task.\n",
    "    - A better approach is to create DummyOperator that does nothing but join both branches and become the upstream task before `join_datasets` such as below: <img src=\"images/branching-v2.png\">\n",
    "    ```python\n",
    "        join_branch = DummyOperator(\n",
    "           task_id=\"join_erp_branch\",\n",
    "           trigger_rule=\"none_failed\"\n",
    "            )\n",
    "       \n",
    "        start >> [pick_erp_system, fetch_weather]\n",
    "        pick_erp_system >> [fetch_sales_old, fetch_sales_new]\n",
    "        fetch_sales_old >> clean_sales_old\n",
    "        fetch_sales_new >> clean_sales_new\n",
    "        [clean_sales_old, clean_sales_new] >> join_branch\n",
    "        fetch_weather >> clean_weather\n",
    "        [joen_erp_branch, clean_weather] >> join_datasets\n",
    "        join_datasets >> train_model\n",
    "        train_model >> deploy_model\n",
    "    ```\n",
    "- **Conditional tasks**. Sometimes we only want to execute a task if a condition is true, otherwise, the task should be skipped. For example, if we want to only deploy the model on the most recent data and we don't want `deploy_model` to always execute if we are doing **backfilling** -> Create a conditional upstream task that checks the condition and raise Exception if the condition is False so `deploy_model` will be skipped. \n",
    "<img src=\"images/conditional-task.png\">\n",
    "\n",
    "```python\n",
    "    from airflow.exceptions import AirflowSkipException\n",
    "    from airflow.operators.python import PythonOperator\n",
    "    def _latest_only(**context):\n",
    "        # execution_time is the first time in the schedule interval\n",
    "        # So following_schedule is the next execution_date\n",
    "        left_window = context[\"dag\"].following_schedule(context[\"execution_date\"])\n",
    "        right_window = context[\"dag\"].following_schedule(left_window)\n",
    "        now = pendulum.now(\"utc\")\n",
    "        # Since execution of DAG starts after last time point passed of the \n",
    "        # schedule interval -> \n",
    "        if not left_window < now <= right_window:\n",
    "            raise AirflowSkipException(\"Not the most recent run!\")\n",
    "\n",
    "    latest_only = PythonOperator(task_id=\"latest_only\", python_callable=_latest_only, dag=dag)\n",
    "    latest_only >> deplpy_model\n",
    "```\n",
    "- **Trigger rules**: The triggering of Airflow tasks is controlled by the trigger rules which define the behavior of tasks and allow us to configure each task to respond to different situations.\n",
    "    1. Be default, scheduler picks tasks ready to be executed when all its upstreams tasks were executed successfully and put it in the execute queue. The scheduler always checks downstream tasks if they are ready by checking all their downstream task completion state. Once there is a slot/worker, it will be executed. If any of the upstream tasks failed, it would have failed state and the upstream task won't be scheduled and have **state=upstream_failed**. This is called **progagation** because the error is propagated from upstream to downstream tasks. This is the default trigger_rule which is **all_success**. If any of the down\n",
    "    2. If any of the upstream task is skipped -> downstream task will be skipped as well (propagation).\n",
    "    3. Trigger rules:\n",
    "        - `all_success`: Triggers when all parent tasks have executed successfully\n",
    "        - `all_failed`: Triggers when all parent tasks have failed or due to failure in their parents\n",
    "        - `all_done`: Triggers when all parent tasks finished executing regardless of their state. Good to cleanup and shutdown resources regardless of the execution state of the workflow\n",
    "        - `one_failed`: Triggers when at least 1 parent task failed and doesn't wait for other parent tasks to finish\n",
    "        - `one_success`: Triggers when at least 1 parent task succeeded and doesn't wait for other parent tasks to finish\n",
    "        - `none_failed`: Triggers if no parent task has failed but either completed successfully or skipped\n",
    "        - `none_skipped`: Triggers if no parent task has skipped but either completed successfully or failed\n",
    "        - `dummy`: Triggers regardless of the parent tasks state. Useful for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b3df5-d2d8-4f10-806a-b8b4d9ee1574",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sharing data between tasks using XComs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3638d166-331d-40b0-a727-94a68dc4cde5",
   "metadata": {},
   "source": [
    "- It is meant to exchange messages between tasks, which is some form of shared state\n",
    "- We can use dag instance to push/pull data between tasks:\n",
    "    - `conext[\"dag_instance\"].xcom_push(key=\"data_name\", value=\"value\")` to push data to metastore. It also store the `dag_id`, `task_id`, & `execution_date`.\n",
    "    - `conext[\"dag_instance\"].xcom_pull(key=\"data_name\")` which pull the shared data. We can also specify `dag_id` and `execution_date`.\n",
    "    - We can also access push/pull methods in templates using `task_instance.xcom_push()` or `task_instance.xcom_pull()`\n",
    "    - We can view the shared data on the UI by going to Admin -> XComs\n",
    "\n",
    "- **Limitations**:\n",
    "    - XComs data will be pickled and stored in the database -> The objects have to be serializable\n",
    "    - Size limitations:\n",
    "        - SQLite—Stored as BLOB type, 2GB limit\n",
    "        - PostgreSQL—Stored as BYTEA type, 1 GB limit \n",
    "        - MySQL—Stored as BLOB type, 64 KB limit\n",
    "    - It create hidden dependency between tasks because now the task the pushes the shared state has to push the data before the task that pulls the data. Airflow won't manage/respect this dependency the developer has to document this and make sure this is not an issue based on the tasks' order\n",
    "- Due to its limitations in terms of size, we can create custom backends for XComs by defining a class that inherits from `BaseXCom` and implements two static methods. Airflow will use this class. It can be added to `xcom_backend` parameter in the Airflow configWe can use cheap/large storage services on the cloud such as Amazon S3, Azure Blob Storage, or Google GCS.\n",
    "\n",
    "```python\n",
    "from typing import Any\n",
    "from airflow.models.xcom import BaseXCom\n",
    "\n",
    "class CustomXComBackend(BaseXCom):\n",
    "    \n",
    "    @staticmethod\n",
    "    def serialize(value: Any):\n",
    "        ...\n",
    "    \n",
    "    @staticmethod\n",
    "    def deserialize(result):\n",
    "        ...\n",
    "```\n",
    "- If most of tasks are PythonOperators, we can use `Taskflow` API that takes care of passing state between tasks and avoid the boilerplate code that we have to write with regular API. We need to just decorate the function that we use in the PythonOperator with `@task` and Airflow will take care of the rest by passed XCom data between tasks. Example:\n",
    "\n",
    "<img src=\"images/taskflow-example.png\">\n",
    "\n",
    "```python\n",
    "from airflow.decorators import task\n",
    "\n",
    "\n",
    "with DAG(...) as dag:\n",
    "    start = DummyOperator(task_id=\"start\")\n",
    "    start >> fetch_sales\n",
    "    start >> fetch_weather\n",
    "    fetch_sales >> clean_sales\n",
    "    fetch_weather >> clean_weather\n",
    "    [clean_sales, clean_weather] >> join_datasets\n",
    "    \n",
    "    @task\n",
    "    def train_model():\n",
    "        model_id = str(uuid.uuid4())\n",
    "        # Airflow will figure out that the return value is XCom\n",
    "        # and would take care of pushing it\n",
    "        return model_id\n",
    "\n",
    "    @task\n",
    "    def deploy_model(model_id: str):\n",
    "        # Airflow would realize that this task uses XCom so it passes\n",
    "        # it from XCom\n",
    "        print(f\"Deploying model {model_id}\")\n",
    "\n",
    "model_id = train_model()\n",
    "deploy_model(model_id)\n",
    "\n",
    "# Now train_model and deploy_model will be new tasks\n",
    "# with explicit dependeny. \n",
    "# The task type is PythonDecoratedOperator\n",
    "join_datasets >> model_id\n",
    "```\n",
    "- Any data passed between Taskflow-style tasks will be stored as XComs and subject to the same limitations of XCom\n",
    "- The main limitation of Taskflow API is that it is still only for PythonOperators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a806e5-3a4b-42ba-ac79-f1fa2992b106",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Triggering Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794fa8c7-b38b-4c2b-9b8f-3269465ebe9f",
   "metadata": {},
   "source": [
    "- Workflows are most commonly triggered based on schedule intervals provided using `start_date`, `end_date` , `schedule_interval`. Airflow would calculate when the next schedule would be and start the first task(s) to run at the next data/time.\n",
    "- However, sometimes we want the workflow to run based on the occurance of external events such as a file is available in specific location OR code is changed on git repo etc.\n",
    "- One way to execute workflows based on the occurance of external exents is using Airflow's **sensors**. Sensor is a subclass of operators that checks if certain condition is true. If true, execute the step (workflow). If false, wait for a given period (default 60 seconds) and tries again. It keeps doing so for *timeout* period. This is a form of **Poking**, which is checking for the existence of file in the case of FileSensor.\n",
    "\n",
    "```Python\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "wait_for_file_1 = FileSensor(\n",
    "    task_id=\"wait_for_file_1\", filepath=\"/data/file_1.csv\"\n",
    "    )\n",
    "```\n",
    "- We can also use **globbing** with FileSensors by using wildcards to check for the existence of file(s)\n",
    "- We can also use PythonSensor which checks for certain condition and must return a Boolean. It is more flexible and easier to read than using globbing within FileSensor. It is the same as PythonOperator in terms of taking a Python callable\n",
    "\n",
    "```Python\n",
    "from pathlib import Path\n",
    "from airflow.sensors.python import PythonSensor\n",
    "\n",
    "# Check whether there is any data for a given supermarker\n",
    "# and there is _SUCCESS path which indicates whether the \n",
    "# data for the given supermarket is all uploaded\n",
    "def _wait_for_supermarket(supermarket):\n",
    "    supermarket_path = Path(\"/data\") / supermarket\n",
    "    success_path = Path(\"/data\") / \"_SUCCESS\"\n",
    "    data_files = supermarketpath.glob(\"*.csv\")\n",
    "    return data_files and success_path.exists()\n",
    "\n",
    "wait_for_supermarket_1 = PythonSensor(\n",
    "    task_id=\"wait_for_supermarket_1\",\n",
    "    python_callable=_wait_for_supermarket,\n",
    "    op_kwargs={\"supermarket\": \"supermarket_1\"},\n",
    "    dag=dag\n",
    "    )\n",
    "```\n",
    "<img src=\"images/python-sensor.png\">\n",
    "\n",
    "- All sensors take a `timeout` arguments, which has default value of 7 days\n",
    "- There is also a limit on the number of tasks Airflow can run concurrently per DAG (default is 16). DAG takes `concurrency` argument that can change this number. There is also a limit on the number of tasks per global Airflow and the number DAG runs per DAG\n",
    "```Python\n",
    "wait_for_supermarket_1 = PythonSensor(\n",
    "    task_id=\"wait_for_supermarket_1\",\n",
    "    python_callable=_wait_for_supermarket,\n",
    "    op_kwargs={\"supermarket\": \"supermarket_1\"},\n",
    "    concurreny=20, # Default is 16\n",
    "    dag=dag\n",
    "    )\n",
    "```\n",
    "- There is snowball effect when sensors don't succeed. The occupy slots that DAG has (which is determined by the concurrency argument. From the above figure, if only task 1 succeeds and the rest keeps polling and the DAG is scheduled daily with default concurrency of 16 slots and default timeout of 7 days, this is what will happen (sensor deadlock):\n",
    "    - Day 1: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 3 tasks.\n",
    "    - Day 2: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 6 tasks.\n",
    "    - Day 3: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 9 tasks.\n",
    "    - Day 4: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 12 tasks.\n",
    "    - Day 5: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 15 tasks.\n",
    "    - Day 6: Supermarket 1 succeeded; supermarkets 2, 3, and 4 are polling, occupying 16 tasks; two new tasks cannot run, and any other task trying to run is blocked.\n",
    "\n",
    "<img src=\"images/python-sensor-deadlock.png\">\n",
    "\n",
    "- This also affect the global Airflow limit of maximum number of tasks that can run concurrently, which may lead to whole system get stalled. \n",
    "- For sensor task, it pokes to check the condition and block if it is false. So it would run for a little bit and wait for the most part. It keeps poking untel the timeout period is completed, which means it keeps occupying the slot until the condition becomes true or timeout is reached\n",
    "- `mode` argument which has two values: {`poking`, `reschedule`}. The default is poking. Reschedule can solve the sensor deadlock and snowball effect because it releases the slot the sensor task is occupying after the slot has finished poking. In other words, sensor task would poke, if condition if false, the system will reschedule it and take its slot and make it available to other tasks. It is the same concept as **process scheduling** that the OS does when a process does a blocking system call.\n",
    "\n",
    "```Python\n",
    "wait_for_supermarket_1 = PythonSensor(\n",
    "    task_id=\"wait_for_supermarket_1\",\n",
    "    python_callable=_wait_for_supermarket,\n",
    "    op_kwargs={\"supermarket\": \"supermarket_1\"},\n",
    "    mode=\"reschedule\",\n",
    "    dag=dag\n",
    "    )\n",
    "```\n",
    "- We can trigger another DAG to run from inside another DAG using `TriggerDagRunOperator`. This will cause another DAG to run once the trigger_operator runs which is useful if we want to split DAGs and make some DAGs available to other DAGs instead of repearing functionality. See below for both approaches:\n",
    "<img src=\"images/complicated-dag-logic.png\">\n",
    "<img src=\"images/triggered-dag.png\">\n",
    "\n",
    "```Python\n",
    "from pathlib import Path\n",
    "\n",
    "import airflow.utils.dates\n",
    "from airflow import DAG\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from airflow.operators.trigger_dagrun import TriggerDagRunOperator\n",
    "from airflow.sensors.python import PythonSensor\n",
    "\n",
    "dag1 = DAG(\n",
    "    dag_id=\"ingest_supermarket_data\",\n",
    "    start_date=airflow.utils.dates.days_ago(3),\n",
    "    schedule_interval=\"0 16 * * *\",\n",
    ")\n",
    "dag2 = DAG(\n",
    "    dag_id=\"create_metrics\",\n",
    "    start_date=airflow.utils.dates.days_ago(3),\n",
    "    schedule_interval=None, # Since it will be triggered\n",
    ")\n",
    "\n",
    "\n",
    "def _wait_for_supermarket(supermarket_id_):\n",
    "    supermarket_path = Path(\"/data/\" + supermarket_id_)\n",
    "    data_files = supermarket_path.glob(\"data-*.csv\")\n",
    "    success_file = supermarket_path / \"_SUCCESS\"\n",
    "    return data_files and success_file.exists()\n",
    "\n",
    "\n",
    "for supermarket_id in range(1, 5):\n",
    "    wait = PythonSensor(\n",
    "        task_id=f\"wait_for_supermarket_{supermarket_id}\",\n",
    "        python_callable=_wait_for_supermarket,\n",
    "        op_kwargs={\"supermarket_id_\": f\"supermarket{supermarket_id}\"},\n",
    "        dag=dag1,\n",
    "    )\n",
    "    copy = DummyOperator(task_id=f\"copy_to_raw_supermarket_{supermarket_id}\", dag=dag1)\n",
    "    process = DummyOperator(task_id=f\"process_supermarket_{supermarket_id}\", dag=dag1)\n",
    "    trigger_create_metrics_dag = TriggerDagRunOperator(\n",
    "        task_id=f\"trigger_create_metrics_dag_supermarket_{supermarket_id}\",\n",
    "        trigger_dag_id=\"create_metrics\", # Has to be the same dag_id as dag2\n",
    "        dag=dag1,\n",
    "    )\n",
    "    wait >> copy >> process >> trigger_create_metrics_dag\n",
    "\n",
    "compute_differences = DummyOperator(task_id=\"compute_differences\", dag=dag2)\n",
    "update_dashboard = DummyOperator(task_id=\"update_dashboard\", dag=dag2)\n",
    "notify_new_data = DummyOperator(task_id=\"notify_new_data\", dag=dag2)\n",
    "compute_differences >> update_dashboard\n",
    "\n",
    "```\n",
    "- Each DAG run has a run_id that starts with one of the following:\n",
    "    - `scheduled__` to indicate the DAG run started because of its schedule\n",
    "    - `backfill__` to indicate the DAG run started by a backfill job\n",
    "    - `manual__` to indicate the DAG run started by a manual action (e.g., pressing the Trigger Dag button, or triggered by a TriggerDagRunOperator)\n",
    "- From the UI, scheduled DAGs have their task instance in black border while Triggered DAGs don't\n",
    "- Clearing a task in a DAG will clear the task and all its downstream tasks and trigger a run (backfill)\n",
    "    - It only clears tasks within the same DAG, NOT downstream tasks in another DAG of TriggerDagRunOperator\n",
    "- If the triggered DAG has dependency on multiple triggering DAGs to be completed before it can run, then we can use `ExternalTaskSensor` that checks whether the task has been completed successfully (sensor poking the state of tasks in another DAGs). Each `ExternalTaskSensor` checks for only 1 task by querying the metastore database\n",
    "    - By default, it uses the same execution_date as itself\n",
    "    - If the task runs on different schedule, we then need to provide timedelta object to `execution_delta` argument to get what would be the execution_date of the task it tries to sense\n",
    "\n",
    "```Python\n",
    "import datetime\n",
    "\n",
    "import airflow.utils.dates\n",
    "from airflow import DAG\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from airflow.sensors.external_task import ExternalTaskSensor\n",
    "\n",
    "dag1 = DAG(\n",
    "    dag_id=\"ingest_supermarket_data\",\n",
    "    start_date=airflow.utils.dates.days_ago(3),\n",
    "    schedule_interval=\"0 16 * * *\",\n",
    ")\n",
    "dag2 = DAG(\n",
    "    dag_id=\"create_metrics\",\n",
    "    start_date=airflow.utils.dates.days_ago(3),\n",
    "    schedule_interval=\"0 18 * * *\",\n",
    ")\n",
    "\n",
    "DummyOperator(task_id=\"copy_to_raw\", dag=dag1) >> DummyOperator(\n",
    "    task_id=\"process_supermarket\", dag=dag1\n",
    ")\n",
    "\n",
    "wait = ExternalTaskSensor(\n",
    "    task_id=\"wait_for_process_supermarket\",\n",
    "    external_dag_id=\"figure_6_20_dag_1\",\n",
    "    external_task_id=\"process_supermarket\",\n",
    "    # positive # will be subtracted from the execution_date of task sensor\n",
    "    # to get the execution_date of the task it is trying to sense\n",
    "    execution_delta=datetime.timedelta(hours=6),  \n",
    "    dag=dag2,\n",
    ")\n",
    "report = DummyOperator(task_id=\"report\", dag=dag2)\n",
    "wait >> report\n",
    "```\n",
    "- We can also trigger DAGs from CLI which will have execution_date of the current data and time\n",
    "    - `airflow dags trigger dag1`\n",
    "    - With configuration; which will be available in the context of each task using context[\"dag_run\"].conf:\n",
    "        - `airflow dags trigger -c '{\"supermarket_id\": 1}' dag1`\n",
    "        - `airflow dags trigger --conf '{\"supermarket_id\": 1}' dag1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ae3e81-d078-4e7a-97ca-03b19f4d8daf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Working with External Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280aa38b-d81a-4a6e-93f6-66739afcc742",
   "metadata": {},
   "source": [
    "- Airflow makes it easy to work with external systems through operators. Most systems such as AWS or PostgreSQL have their own operators (classes) that provide the functionality and hide the technical implementation inside the API\n",
    "- Airflow will store logs and other information in the directory `AIRFLOW_HOME`. It also looks for dags inside this directory. If dags are in different directory, we can set `AIRFLOW__CORE_DAGS_FOLDER`\n",
    "- Airflow default metastore is inside `AIRFLOW_HOME` and stores all information inside a file name `airflow.db`. We can point Airflow to store information inside another database by setting `AIRFLOW_CORE_SQL_ALCHEMY_CONN` (URI for the database)\n",
    "- We can test 1 task from workflow using `airflow tasks test dag_name task_name execution_date`\n",
    "    - This will run the task locally and display the logs but it doesn't store the state of the task in the metastore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151f1890-6206-483b-84a1-4e2a994d5196",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11931e28-7ba8-4233-87e0-8558e01614ed",
   "metadata": {},
   "source": [
    "- Stick to coding style conventions by using tools like flake8, pylint, black\n",
    "- There are two ways to define DAGs. Stick to one of them:\n",
    "    1. With context manager: \n",
    "```Python\n",
    "   with DAG(...) as dag:\n",
    "        task1 = PythonOperator(...)\n",
    "        task2 = PythonOperator(...)\n",
    "```\n",
    "    2. Traditional:\n",
    "```Python\n",
    "   dag = DAG(...)\n",
    "   task1 = PythonOperator(..., dag=dag)\n",
    "   task2 = PythonOperator(..., dag=dag)\n",
    "```\n",
    "- There are also multiple ways to define dependencies. Stick to one of them:\n",
    "```Python\n",
    "    task1 >> task2\n",
    "    task1 << task2\n",
    "    [task1] >> task2\n",
    "    task1.set_downstream(task2)\n",
    "    task2.set_upstream(task1)\n",
    "```\n",
    "- When loading config files, make sure to understand where the loading happens:\n",
    "    - At the top level on the scheduler\n",
    "    - At the DAG level when it is parsed\n",
    "    - Or when the DAG is executing -> in the worker\n",
    "- Avoid doing any computation in DAG definition:\n",
    "    - At the top level, it will be computed every time the DAG is loaded\n",
    "    - In the DAG definition, it will be executed every time the DAG is parsed by the scheduler\n",
    "    - In the task, it will be computed when the task is executed on the worker machine\n",
    "- Fetch credentials within the task, so they are only fetched once the task is executed\n",
    "- Use factory methods to generate DAGs or set of tasks that are almost typical with few minor changes. Example:\n",
    "\n",
    "```Python\n",
    "def generate_tasks(dataset_name, raw_dir, processed_dir, preprocess_script, output_dir, dag):\n",
    "    raw_path = os.path.join(raw_dir, dataset_name, \"{ds_nodash}.json\") \n",
    "    processed_path = os.path.join(\n",
    "    processed_dir, dataset_name, \"{ds_nodash}.json\" )\n",
    "    output_path = os.path.join(output_dir, dataset_name, \"{ds_nodash}.json\")\n",
    "    fetch_task = BashOperator(\n",
    "        task_id=f\"fetch_{dataset_name}\",\n",
    "        bash_command=f\"echo 'curl http://example.com/{dataset_name}.json{raw_path}.json'\", dag=dag,\n",
    "        )\n",
    "    preprocess_task = BashOperator(\n",
    "        task_id=f\"preprocess_{dataset_name}\",\n",
    "        bash_command=f\"echo '{preprocess_script} {raw_path} {processed_path}'\", dag=dag,\n",
    "    )\n",
    "    export_task = BashOperator(\n",
    "        task_id=f\"export_{dataset_name}\",\n",
    "        bash_command=f\"echo 'cp {processed_path} {output_path}'\", dag=dag,\n",
    "       )\n",
    "        fetch_task >> preprocess_task >> export_task\n",
    "    return fetch_task, export_task\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"01_task_factory\",\n",
    "    start_date=airflow.utils.dates.days_ago(5),\n",
    "    schedule_interval=\"@daily\",\n",
    ") as dag:\n",
    "    for dataset in [\"sales\", \"customers\"]:\n",
    "        generate_tasks(\n",
    "            dataset_name=dataset,\n",
    "            raw_dir=\"/data/raw\", \n",
    "            processed_dir=\"/data/processed\", \n",
    "            output_dir=\"/data/output\",\n",
    "            preprocess_script=f\"preprocess_{dataset}.py\", dag=dag\n",
    "        )\n",
    "```\n",
    "- We can use `TaskGroup` to group related tasks into groups that will help us navigating the DAG in the UI. This is very helpful when DAGs become very complicated\n",
    "- Create new DAGs for big changes such as renaming/removing tasks or changing the schedule_date/interval so we can keep the historical info about old DAGs and not confuse the scheduler. Scheduler database has instances of the runs of each DAG\n",
    "- Make sure that tasks are idempotenet -> Regardless when they run, If given the same input the should produce the same output. Therefore, be careful when writing data. We may want to overwrite or upsert to avoid appending the same data\n",
    "    - Also, tasks should not have side effects\n",
    "- Avoid writing intermediate results on local filesystem because each task runs independently (and mostly on different machines) -> Use cloud shared storage such as Amazon's S3 bucket where all workers can access it\n",
    "- We can use SLAs on each DAG/task where Airflow will notify if they don't finish within SLA. DAG takes `sla` argument"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
